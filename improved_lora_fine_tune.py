import torch
import json
import pandas as pd
import numpy as np
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    BitsAndBytesConfig,
    TrainingArguments, 
    Trainer, 
    DataCollatorForLanguageModeling,
    TrainerCallback
)
from peft import (
    prepare_model_for_kbit_training,
    LoraConfig,
    get_peft_model,
    PeftModel,
    PeftConfig
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
import gc
import os
from typing import Dict, List, Tuple
import re

# Improved configuration
class Config:
    HF_TOKEN = "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"
    MODEL_NAME = "CohereLabs/aya-23-8B"
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    MAX_LENGTH = 512
    OUTPUT_DIR = "./medical_lora_output"
    FINAL_MODEL_DIR = "./final_medical_lora"

def load_base_model():
    """Enhanced model loading with better error handling"""
    print(f"Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ {Config.MODEL_NAME}...")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            Config.MODEL_NAME, 
            trust_remote_code=True, 
            token=Config.HF_TOKEN,
            use_fast=True  # Use fast tokenizer for better performance
        )
        
        # Better padding configuration
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "right"

        # Optimized quantization config
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_quant_storage=torch.uint8  # Better storage efficiency
        )

        model = AutoModelForCausalLM.from_pretrained(
            Config.MODEL_NAME,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            token=Config.HF_TOKEN,
            torch_dtype=torch.bfloat16,  # Consistent dtype
            low_cpu_mem_usage=True  # Better memory management
        )

        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)
        return model, tokenizer
    
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: {e}")
        raise

def prepare_training_data_enhanced(data_path: str, tokenizer, validation_split: float = 0.15):
    """Enhanced data preparation with better validation split and formatting"""
    
    print("Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ...")
    
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    utterances = data.get("Utterance", [])
    formatted_data = []
    labels = []  # For evaluation purposes

    for item in utterances:
        query = item.get("query", "")
        diseases = item.get("diseases", [])
        
        if not query or not diseases:
            continue
            
        diseases_text = "ØŒ ".join(diseases)
        
        # Enhanced prompt template with better structure
        formatted_text = f"""<|system|>
Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù¾Ø²Ø´Ú©ÛŒ Ù…ØªØ®ØµØµ Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ù„Ø§Ø¦Ù… Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ØŒ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­ØªÙ…Ù„ Ø±Ø§ ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯. Ù¾Ø§Ø³Ø® Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø¹Ù„Ù…ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.

<|user|>
{query}

<|assistant|>
Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø¹Ù„Ø§Ø¦Ù… Ø°Ú©Ø± Ø´Ø¯Ù‡ØŒ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­ØªÙ…Ù„ Ø¹Ø¨Ø§Ø±ØªÙ†Ø¯ Ø§Ø²: {diseases_text}

Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±ØŒ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø¨Ø§ Ù¾Ø²Ø´Ú© Ù…ØªØ®ØµØµ Ù…Ø´ÙˆØ±Øª Ú©Ù†ÛŒØ¯ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯.<|end|>"""
        
        formatted_data.append({"text": formatted_text})
        labels.append(diseases)

    # Create dataset with labels for evaluation
    df = pd.DataFrame({"text": [item["text"] for item in formatted_data], "labels": labels})
    dataset = Dataset.from_pandas(df)

    def tokenize_function(examples):
        tokenized = tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=Config.MAX_LENGTH,
            return_tensors="pt"
        )
        
        # Add labels column for evaluation
        tokenized["labels"] = tokenized["input_ids"].clone()
        return tokenized

    tokenized_dataset = dataset.map(
        tokenize_function, 
        batched=True,
        remove_columns=dataset.column_names
    )

    # Better train/validation split
    split_dataset = tokenized_dataset.train_test_split(
        test_size=validation_split,
        seed=42,
        shuffle=True
    )

    return split_dataset

def create_optimized_lora_config():
    """Optimized LoRA configuration for better performance"""
    
    return LoraConfig(
        r=8,  # Increased rank for better capacity
        lora_alpha=32,  # Proper scaling
        lora_dropout=0.1,  # Slightly higher dropout for regularization
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=[
            "q_proj",
            "k_proj", 
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj"  # Include more modules for better adaptation
        ],
        modules_to_save=["embed_tokens", "lm_head"]  # Save important modules
    )

class MetricsCallback(TrainerCallback):
    """Enhanced callback for better monitoring"""
    
    def __init__(self):
        self.training_losses = []
        self.eval_losses = []
        
    def on_log(self, args, state, control, model=None, logs=None, **kwargs):
        if logs:
            if "train_loss" in logs:
                self.training_losses.append(logs["train_loss"])
            if "eval_loss" in logs:
                self.eval_losses.append(logs["eval_loss"])
            
            # Memory monitoring
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated() / 1024**3
                print(f"GPU Memory: {allocated:.2f}GB")

def train_model_enhanced(model, tokenizer, tokenized_dataset, lora_config):
    """Enhanced training with better monitoring and optimization"""
    
    print("Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡...")
    torch.cuda.empty_cache()
    gc.collect()
    
    print("Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„ LoRA...")
    
    # Enable gradient checkpointing
    model.gradient_checkpointing_enable()
    peft_model = get_peft_model(model, lora_config)
    peft_model.print_trainable_parameters()

    # Enhanced training arguments
    training_args = TrainingArguments(
        output_dir=Config.OUTPUT_DIR,
        num_train_epochs=5,  # Increased epochs
        per_device_train_batch_size=1,
        per_device_eval_batch_size=2,
        gradient_accumulation_steps=16,
        gradient_checkpointing=True,
        
        # Learning rate scheduling
        learning_rate=1e-4,  # Slightly lower learning rate
        lr_scheduler_type="cosine_with_restarts",
        warmup_steps=100,
        
        # Evaluation and saving
        eval_strategy="steps",
        eval_steps=100,
        save_strategy="steps",
        save_steps=200,
        save_total_limit=3,
        
        # Optimization
        fp16=True,
        dataloader_pin_memory=False,
        remove_unused_columns=False,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        
        # Logging
        logging_steps=50,
        logging_first_step=True,
        report_to="none",
        
        # Advanced optimizations
        optim="adamw_torch_fused",  # Faster optimizer
        max_grad_norm=1.0,
        weight_decay=0.01,
        adam_beta1=0.9,
        adam_beta2=0.999,
        
        # Memory optimizations
        include_inputs_for_metrics=False,
        prediction_loss_only=False,  # Enable for metrics calculation
    )

    # Enhanced data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8,
        return_tensors="pt"
    )

    # Callbacks
    metrics_callback = MetricsCallback()

    trainer = Trainer(
        model=peft_model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["test"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        callbacks=[metrics_callback]
    )

    print("Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´...")
    
    try:
        training_result = trainer.train()
        print(f"Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯. Final loss: {training_result.training_loss:.4f}")
        
        # Save model with better configuration
        peft_model.save_pretrained(
            Config.FINAL_MODEL_DIR,
            safe_serialization=True,
            max_shard_size="1GB"
        )
        tokenizer.save_pretrained(Config.FINAL_MODEL_DIR)
        
        # Save training history
        history = {
            "training_losses": metrics_callback.training_losses,
            "eval_losses": metrics_callback.eval_losses,
            "final_loss": training_result.training_loss
        }
        
        with open(f"{Config.FINAL_MODEL_DIR}/training_history.json", "w") as f:
            json.dump(history, f, indent=2)
            
        return peft_model, history
        
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´: {e}")
        raise

def evaluate_model_comprehensive(model_path: str, test_data_path: str = None):
    """Comprehensive model evaluation with multiple metrics"""
    
    print("Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¬Ø§Ù…Ø¹ Ù…Ø¯Ù„...")
    
    # Load model and tokenizer
    config = PeftConfig.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    base_model = AutoModelForCausalLM.from_pretrained(
        config.base_model_name_or_path,
        load_in_4bit=True,
        device_map="auto",
        torch_dtype=torch.bfloat16
    )
    
    model = PeftModel.from_pretrained(base_model, model_path)
    model.eval()

    # Test questions with expected answers for evaluation
    test_cases = [
        {
            "query": "Ù…Ù† Ø³Ø±Ø¯Ø±Ø¯ Ø´Ø¯ÛŒØ¯ØŒ ØªØ¨ Ùˆ Ú¯Ù„ÙˆØ¯Ø±Ø¯ Ø¯Ø§Ø±Ù…",
            "expected_diseases": ["Ø¢Ù†ÙÙˆÙ„Ø§Ù†Ø²Ø§", "Ø³Ø±Ù…Ø§Ø®ÙˆØ±Ø¯Ú¯ÛŒ", "Ø§Ù„ØªÙ‡Ø§Ø¨ Ú¯Ù„Ùˆ"],
            "category": "respiratory"
        },
        {
            "query": "Ø¯Ø±Ø¯ Ù‚ÙØ³Ù‡ Ø³ÛŒÙ†Ù‡ØŒ ØªÙ†Ú¯ÛŒ Ù†ÙØ³ Ùˆ Ø®Ø³ØªÚ¯ÛŒ Ø¯Ø§Ø±Ù…",
            "expected_diseases": ["Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ù‚Ù„Ø¨ÛŒ", "Ø¢Ù†Ú˜ÛŒÙ† ØµØ¯Ø±ÛŒ", "Ù†Ø§Ø±Ø³Ø§ÛŒÛŒ Ù‚Ù„Ø¨ÛŒ"],
            "category": "cardiovascular"
        },
        {
            "query": "Ø¯Ø±Ø¯ Ø´Ú©Ù…ØŒ ØªÙ‡ÙˆØ¹ Ùˆ Ø§Ø³Ù‡Ø§Ù„ Ø¯Ø§Ø±Ù…",
            "expected_diseases": ["Ú¯Ø§Ø³ØªØ±ÙˆØ§Ù†ØªØ±ÛŒØª", "Ø¹ÙÙˆÙ†Øª Ø±ÙˆØ¯Ù‡", "Ù…Ø³Ù…ÙˆÙ…ÛŒØª ØºØ°Ø§ÛŒÛŒ"],
            "category": "gastrointestinal"
        },
        {
            "query": "Ø³Ø±Ø¯Ø±Ø¯ Ù…Ø¯Ø§ÙˆÙ…ØŒ ØªØ§Ø±ÛŒ Ø¯ÛŒØ¯ Ùˆ Ø³Ø±Ú¯ÛŒØ¬Ù‡",
            "expected_diseases": ["ÙØ´Ø§Ø± Ø®ÙˆÙ† Ø¨Ø§Ù„Ø§", "Ù…ÛŒÚ¯Ø±Ù†", "Ù…Ø´Ú©Ù„Ø§Øª Ø¨ÛŒÙ†Ø§ÛŒÛŒ"],
            "category": "neurological"
        },
        {
            "query": "Ø¯Ø±Ø¯ Ù…ÙØ§ØµÙ„ØŒ Ø®Ø´Ú©ÛŒ Ø¯Ù‡Ø§Ù† Ùˆ Ø®Ø³ØªÚ¯ÛŒ",
            "expected_diseases": ["Ø¢Ø±ØªØ±ÛŒØª Ø±ÙˆÙ…Ø§ØªÙˆØ¦ÛŒØ¯", "Ù„ÙˆÙ¾ÙˆØ³", "Ø³Ù†Ø¯Ø±Ù… Ø®Ø´Ú©ÛŒ"],
            "category": "autoimmune"
        }
    ]

    results = {
        "accuracy_scores": [],
        "precision_scores": [],
        "recall_scores": [],
        "f1_scores": [],
        "category_performance": {},
        "detailed_results": []
    }

    for i, test_case in enumerate(test_cases):
        print(f"\nØ§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…ÙˆØ±Ø¯ {i+1}/{len(test_cases)}: {test_case['category']}")
        
        prompt = f"""<|system|>
Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù¾Ø²Ø´Ú©ÛŒ Ù…ØªØ®ØµØµ Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ù„Ø§Ø¦Ù… Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ØŒ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­ØªÙ…Ù„ Ø±Ø§ ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯.

<|user|>
{test_case['query']}

<|assistant|>
Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø¹Ù„Ø§Ø¦Ù… Ø°Ú©Ø± Ø´Ø¯Ù‡ØŒ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­ØªÙ…Ù„ Ø¹Ø¨Ø§Ø±ØªÙ†Ø¯ Ø§Ø²:"""

        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=400)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.3,  # Lower temperature for more consistent results
                do_sample=True,
                top_p=0.8,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_text = response.split("<|assistant|>")[-1].strip()
        
        # Extract predicted diseases using regex
        disease_pattern = r'(?:Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­ØªÙ…Ù„|Ø¹Ø¨Ø§Ø±ØªÙ†Ø¯ Ø§Ø²)[:\s]*([^\.]+)'
        match = re.search(disease_pattern, generated_text)
        
        if match:
            predicted_diseases_text = match.group(1)
            predicted_diseases = [disease.strip() for disease in predicted_diseases_text.split('ØŒ')]
        else:
            predicted_diseases = []

        # Calculate metrics
        accuracy = calculate_disease_accuracy(predicted_diseases, test_case['expected_diseases'])
        precision, recall, f1 = calculate_disease_metrics(predicted_diseases, test_case['expected_diseases'])
        
        results["accuracy_scores"].append(accuracy)
        results["precision_scores"].append(precision)
        results["recall_scores"].append(recall)
        results["f1_scores"].append(f1)
        
        # Store category performance
        category = test_case['category']
        if category not in results["category_performance"]:
            results["category_performance"][category] = []
        results["category_performance"][category].append({
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1": f1
        })
        
        # Detailed results
        detailed_result = {
            "query": test_case['query'],
            "expected": test_case['expected_diseases'],
            "predicted": predicted_diseases,
            "full_response": generated_text,
            "metrics": {"accuracy": accuracy, "precision": precision, "recall": recall, "f1": f1}
        }
        results["detailed_results"].append(detailed_result)
        
        print(f"Ø³ÙˆØ§Ù„: {test_case['query']}")
        print(f"Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„: {generated_text}")
        print(f"Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡: {predicted_diseases}")
        print(f"Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±: {test_case['expected_diseases']}")
        print(f"Ø¯Ù‚Øª: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}")
        print("-" * 80)

    # Calculate overall metrics
    overall_metrics = {
        "mean_accuracy": np.mean(results["accuracy_scores"]),
        "mean_precision": np.mean(results["precision_scores"]),
        "mean_recall": np.mean(results["recall_scores"]),
        "mean_f1": np.mean(results["f1_scores"]),
        "std_accuracy": np.std(results["accuracy_scores"]),
        "std_precision": np.std(results["precision_scores"]),
        "std_recall": np.std(results["recall_scores"]),
        "std_f1": np.std(results["f1_scores"])
    }

    # Category-wise performance
    for category, performances in results["category_performance"].items():
        category_metrics = {
            "accuracy": np.mean([p["accuracy"] for p in performances]),
            "precision": np.mean([p["precision"] for p in performances]),
            "recall": np.mean([p["recall"] for p in performances]),
            "f1": np.mean([p["f1"] for p in performances])
        }
        results["category_performance"][category] = category_metrics

    results["overall_metrics"] = overall_metrics

    # Print comprehensive results
    print("\n" + "="*50)
    print("ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¬Ø§Ù…Ø¹")
    print("="*50)
    print(f"Ø¯Ù‚Øª Ú©Ù„ÛŒ: {overall_metrics['mean_accuracy']:.3f} Â± {overall_metrics['std_accuracy']:.3f}")
    print(f"Precision Ú©Ù„ÛŒ: {overall_metrics['mean_precision']:.3f} Â± {overall_metrics['std_precision']:.3f}")
    print(f"Recall Ú©Ù„ÛŒ: {overall_metrics['mean_recall']:.3f} Â± {overall_metrics['std_recall']:.3f}")
    print(f"F1-Score Ú©Ù„ÛŒ: {overall_metrics['mean_f1']:.3f} Â± {overall_metrics['std_f1']:.3f}")
    
    print("\nğŸ“ˆ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ:")
    for category, metrics in results["category_performance"].items():
        print(f"{category}: Acc={metrics['accuracy']:.3f}, P={metrics['precision']:.3f}, "
              f"R={metrics['recall']:.3f}, F1={metrics['f1']:.3f}")

    # Save results
    with open(f"{Config.FINAL_MODEL_DIR}/evaluation_results.json", "w", encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    return results

def calculate_disease_accuracy(predicted: List[str], expected: List[str]) -> float:
    """Calculate accuracy for disease prediction"""
    if not expected:
        return 1.0 if not predicted else 0.0
    
    # Normalize disease names for comparison
    predicted_normalized = [disease.strip().lower() for disease in predicted]
    expected_normalized = [disease.strip().lower() for disease in expected]
    
    # Calculate intersection
    correct_predictions = len(set(predicted_normalized) & set(expected_normalized))
    total_expected = len(expected_normalized)
    
    return correct_predictions / total_expected if total_expected > 0 else 0.0

def calculate_disease_metrics(predicted: List[str], expected: List[str]) -> Tuple[float, float, float]:
    """Calculate precision, recall, and F1 score for disease prediction"""
    if not expected and not predicted:
        return 1.0, 1.0, 1.0
    if not expected:
        return 0.0, 1.0, 0.0
    if not predicted:
        return 1.0, 0.0, 0.0
    
    # Normalize disease names
    predicted_normalized = set(disease.strip().lower() for disease in predicted)
    expected_normalized = set(disease.strip().lower() for disease in expected)
    
    # Calculate metrics
    true_positives = len(predicted_normalized & expected_normalized)
    false_positives = len(predicted_normalized - expected_normalized)
    false_negatives = len(expected_normalized - predicted_normalized)
    
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return precision, recall, f1

def interactive_test(model_path: str):
    """Interactive testing interface"""
    
    print("Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ ØªØ³Øª ØªØ¹Ø§Ù…Ù„ÛŒ...")
    
    config = PeftConfig.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    base_model = AutoModelForCausalLM.from_pretrained(
        config.base_model_name_or_path,
        load_in_4bit=True,
        device_map="auto"
    )
    
    model = PeftModel.from_pretrained(base_model, model_path)
    model.eval()
    
    print("\nğŸ©º Ø¯Ø³ØªÛŒØ§Ø± Ù¾Ø²Ø´Ú©ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª! (Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ 'exit' ØªØ§ÛŒÙ¾ Ú©Ù†ÛŒØ¯)")
    
    while True:
        user_input = input("\nØ¹Ù„Ø§Ø¦Ù… Ø®ÙˆØ¯ Ø±Ø§ Ø´Ø±Ø­ Ø¯Ù‡ÛŒØ¯: ").strip()
        
        if user_input.lower() in ['exit', 'Ø®Ø±ÙˆØ¬', 'quit']:
            break
            
        if not user_input:
            continue
        
        prompt = f"""<|system|>
Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù¾Ø²Ø´Ú©ÛŒ Ù…ØªØ®ØµØµ Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ù„Ø§Ø¦Ù… Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ØŒ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­ØªÙ…Ù„ Ø±Ø§ ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯.

<|user|>
{user_input}

<|assistant|>
Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø¹Ù„Ø§Ø¦Ù… Ø°Ú©Ø± Ø´Ø¯Ù‡ØŒ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­ØªÙ…Ù„ Ø¹Ø¨Ø§Ø±ØªÙ†Ø¯ Ø§Ø²:"""

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=400)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.4,
                do_sample=True,
                top_p=0.85,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_text = response.split("<|assistant|>")[-1].strip()
        
        print(f"\nğŸ©º Ù¾Ø§Ø³Ø® Ø¯Ø³ØªÛŒØ§Ø± Ù¾Ø²Ø´Ú©ÛŒ:\n{generated_text}")

def main():
    """Main execution function"""
    
    # Set environment variables for better performance
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ù¾Ø²Ø´Ú©ÛŒ Ø¨Ø§ Ø¨Ù‡Ø¨ÙˆØ¯Ù‡Ø§ÛŒ Ø¬Ø§Ù…Ø¹")
    
    # Load model and tokenizer
    model, tokenizer = load_base_model()
    
    # Prepare data
    dataset_path = "/kaggle/input/multi-intend-detection/multi_intent_disease_queries(method3).json"
    tokenized_dataset = prepare_training_data_enhanced(dataset_path, tokenizer)
    print(f"ğŸ“Š Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {tokenized_dataset}")
    
    # Create LoRA config
    lora_config = create_optimized_lora_config()
    
    # Train model
    trained_model, history = train_model_enhanced(model, tokenizer, tokenized_dataset, lora_config)
    
    print("âœ… Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯!")
    
    # Comprehensive evaluation
    print("\nğŸ” Ø´Ø±ÙˆØ¹ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¬Ø§Ù…Ø¹...")
    evaluation_results = evaluate_model_comprehensive(Config.FINAL_MODEL_DIR)
    
    # Interactive testing
    print("\nğŸ¯ Ø¢ØºØ§Ø² ØªØ³Øª ØªØ¹Ø§Ù…Ù„ÛŒ...")
    interactive_test(Config.FINAL_MODEL_DIR)
    
    return trained_model, history, evaluation_results

if __name__ == "__main__":
    # Run the improved training and evaluation
    trained_model, training_history, eval_results = main()