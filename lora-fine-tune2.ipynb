{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"995029c99a6a45eeb9847dbda3db816f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b0ba72124aae4e1189365461fa932539","IPY_MODEL_5f5b937c61a4447d9cc64ab520672e02","IPY_MODEL_c195ab1ade6b456db214d08f57c02f72"],"layout":"IPY_MODEL_4081d804c70144a1b51d466cd8936f8c"}},"b0ba72124aae4e1189365461fa932539":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5eae2ce3bee42dd970c88c988161cbd","placeholder":"​","style":"IPY_MODEL_d523d9022ce64aa481a57373f2e6a289","value":"Map: 100%"}},"5f5b937c61a4447d9cc64ab520672e02":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_48ad6a6f3788414bb97087447cccebfe","max":320,"min":0,"orientation":"horizontal","style":"IPY_MODEL_43f04c6fab9e4929bcbbff1396d56d1b","value":320}},"c195ab1ade6b456db214d08f57c02f72":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f737a5b6f0db421abd6318aa439357e8","placeholder":"​","style":"IPY_MODEL_a2d149c299444115aace444f6b71b8ef","value":" 320/320 [00:00&lt;00:00, 740.86 examples/s]"}},"4081d804c70144a1b51d466cd8936f8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5eae2ce3bee42dd970c88c988161cbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d523d9022ce64aa481a57373f2e6a289":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48ad6a6f3788414bb97087447cccebfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43f04c6fab9e4929bcbbff1396d56d1b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f737a5b6f0db421abd6318aa439357e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2d149c299444115aace444f6b71b8ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"473b705452e84904bc1177b5544a612f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7ce6814abc9e431b8a23ff4da9c1168f","IPY_MODEL_3060c56c26b748c0b09181755ffee2e3","IPY_MODEL_bfeeacd4082d4c41884e4ae4a746d3e3"],"layout":"IPY_MODEL_aec800fcbe3045e99637ec6408ed3a0a"}},"7ce6814abc9e431b8a23ff4da9c1168f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a2a097dc176453db900157a0fabbb05","placeholder":"​","style":"IPY_MODEL_4dfbbbc2f5eb46219aaa96bb2cde4b04","value":"Map: 100%"}},"3060c56c26b748c0b09181755ffee2e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ccaf32fa0dd442509f7f48e6becbe6e7","max":965,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fbff2850b2f447b280a6c7993ca226cc","value":965}},"bfeeacd4082d4c41884e4ae4a746d3e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c85ac3225e946d3b5e577970b3b979e","placeholder":"​","style":"IPY_MODEL_dfec3c1e16fd49cfa6bb9767d18485ce","value":" 965/965 [00:01&lt;00:00, 703.82 examples/s]"}},"aec800fcbe3045e99637ec6408ed3a0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a2a097dc176453db900157a0fabbb05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dfbbbc2f5eb46219aaa96bb2cde4b04":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ccaf32fa0dd442509f7f48e6becbe6e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbff2850b2f447b280a6c7993ca226cc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7c85ac3225e946d3b5e577970b3b979e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfec3c1e16fd49cfa6bb9767d18485ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"27722232f83240328f5cdd14f88ff6d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2794f46dd35e4067801d9c110c9d7d77","IPY_MODEL_7b904d600376469da5ca728f4285ece4","IPY_MODEL_bd4a6060f0c84aa6bb69b9ca4e99633a"],"layout":"IPY_MODEL_537ca4e20fd544e68f183576999a3357"}},"2794f46dd35e4067801d9c110c9d7d77":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d05ca9e806064d239a920ceb24ca7471","placeholder":"​","style":"IPY_MODEL_a0ddb6f740e845c6bd23f885537223b1","value":"Map: 100%"}},"7b904d600376469da5ca728f4285ece4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac5e006997e04317b947f6791623c9c4","max":2454,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cb7cccc9c1614f2a887b0438d5bc1289","value":2454}},"bd4a6060f0c84aa6bb69b9ca4e99633a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_184ccc0bc81649b191ab1fd165df0e9b","placeholder":"​","style":"IPY_MODEL_35f2ed0a745f46f0aff4e5846736ba1b","value":" 2454/2454 [00:02&lt;00:00, 826.58 examples/s]"}},"537ca4e20fd544e68f183576999a3357":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d05ca9e806064d239a920ceb24ca7471":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0ddb6f740e845c6bd23f885537223b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac5e006997e04317b947f6791623c9c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb7cccc9c1614f2a887b0438d5bc1289":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"184ccc0bc81649b191ab1fd165df0e9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35f2ed0a745f46f0aff4e5846736ba1b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12778630,"sourceType":"datasetVersion","datasetId":8078701}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##  نصب کتابخانه‌های لازم","metadata":{"id":"d19wo3o1m0pD"}},{"cell_type":"code","source":"!pip install transformers peft datasets accelerate bitsandbytes trl -q\n!pip install sentencepiece protobuf -q\n# !pip install -U bitsandbytes accelerate -q","metadata":{"id":"mz_Mx-41mkaF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e1c214a1-39c9-493c-8b69-eb02355cc4fe","trusted":true,"execution":{"iopub.status.busy":"2025-08-16T09:55:11.048930Z","iopub.execute_input":"2025-08-16T09:55:11.049388Z","iopub.status.idle":"2025-08-16T09:56:53.920752Z","shell.execute_reply.started":"2025-08-16T09:55:11.049368Z","shell.execute_reply":"2025-08-16T09:56:53.919952Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip uninstall transformers -y -q\n!pip install transformers -q\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T09:56:53.921874Z","iopub.execute_input":"2025-08-16T09:56:53.922664Z","iopub.status.idle":"2025-08-16T09:57:05.123011Z","shell.execute_reply.started":"2025-08-16T09:56:53.922626Z","shell.execute_reply":"2025-08-16T09:57:05.122270Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## وارد کردن توکن","metadata":{"id":"2zdPD_jOoTT2"}},{"cell_type":"code","source":"# use_auth_token=\"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\"\nuse_auth_token=\"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\"","metadata":{"id":"7_t_p6QFoWSx","trusted":true,"execution":{"iopub.status.busy":"2025-08-16T09:57:05.124917Z","iopub.execute_input":"2025-08-16T09:57:05.125146Z","iopub.status.idle":"2025-08-16T09:57:05.129047Z","shell.execute_reply.started":"2025-08-16T09:57:05.125112Z","shell.execute_reply":"2025-08-16T09:57:05.128427Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# !huggingface-cli login","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nKuwLfli0ShR","outputId":"11fce2c9-041c-4667-f538-b9cde34410ee","trusted":true,"execution":{"iopub.status.busy":"2025-08-16T09:57:05.130387Z","iopub.execute_input":"2025-08-16T09:57:05.130571Z","iopub.status.idle":"2025-08-16T09:57:05.147368Z","shell.execute_reply.started":"2025-08-16T09:57:05.130557Z","shell.execute_reply":"2025-08-16T09:57:05.146854Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## انتخاب مدل پایه","metadata":{"id":"xriTnOSXm8TY"}},{"cell_type":"code","source":"import torch\nimport os\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training\n\nHF_TOKEN = \"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\"  \n\n# prevent from memory fragmentation\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\nMODEL_NAME = \"CohereLabs/aya-23-8B\"\n# MODEL_NAME = \"meta-llama/Llama-3.1-8B\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ndef load_base_model():\n    print(f\"بارگذاری مدل {MODEL_NAME}...\")\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        MODEL_NAME, \n        trust_remote_code=True, \n        token=HF_TOKEN\n    )\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16\n    )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True,\n        token=HF_TOKEN,\n    )\n\n    model = prepare_model_for_kbit_training(model)\n\n    return model, tokenizer\n# load_base_model()    ","metadata":{"id":"1Yox9LfUlyrp","trusted":true,"execution":{"iopub.status.busy":"2025-08-16T09:57:05.148025Z","iopub.execute_input":"2025-08-16T09:57:05.148266Z","iopub.status.idle":"2025-08-16T09:57:42.115465Z","shell.execute_reply.started":"2025-08-16T09:57:05.148244Z","shell.execute_reply":"2025-08-16T09:57:42.114835Z"}},"outputs":[{"name":"stderr","text":"2025-08-16 09:57:23.183332: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755338243.554434      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755338243.657046      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## آماده‌سازی داده‌های آموزشی","metadata":{"id":"frXTlWJinbR_"}},{"cell_type":"markdown","source":"### for the first method","metadata":{"id":"Ah44V-s3kHxP"}},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom datasets import Dataset\n\ndef prepare_training_data_conversation(data_path, tokenizer):\n    \"\"\"آماده‌سازی داده‌ها برای آموزش مدل مکالمه پزشکی مبتنی بر دیالوگ چند مرحله‌ای\"\"\"\n\n    print(\"آماده‌سازی داده‌های آموزشی...\")\n\n    # بارگذاری داده‌ها\n    with open(data_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n\n    utterances = data.get(\"Utterance\", [])\n\n    formatted_data = []\n\n    for dialogue in utterances:\n        conversation = dialogue.get(\"conversation\", [])\n        for turn in conversation:\n            patient = turn.get('patient', '').strip()\n            physician = turn.get('physician', '').strip()\n            intends = turn.get('intend', [])\n            detection = turn.get('detection', [])\n            prescription = turn.get('prescription', [])\n\n            # تبدیل لیست‌ها به متن\n            intends_text = \"، \".join(intends) if intends else \"\"\n            detection_text = \"، \".join(detection) if detection else \"\"\n            prescription_text = \"، \".join(prescription) if prescription else \"\"\n\n            # فرمت‌بندی پیشرفته برای آموزش چندوظیفه‌ای (اختیاری)\n            # این ساختار را می‌توانید بر اساس نیاز خود تغییر دهید (مثلاً حذف یا اضافه کردن detection/prescription)\n            formatted_text = f\"\"\"\n            ### سیستم:\n            شما یک دستیار پزشکی هستید که باید هم به سوالات و نگرانی‌های بیمار پاسخ دهید و هم نیت، تشخیص و اقدامات لازم را شناسایی کنید.\n\n            ### کاربر:\n            {patient}\n            {\"[نیت: \" + intends_text + \"]\" if intends_text else \"\"}\n            {\"[تشخیص: \" + detection_text + \"]\" if detection_text else \"\"}\n            {\"[اقدامات: \" + prescription_text + \"]\" if prescription_text else \"\"}\n\n            ### پاسخ:\n            {physician}\n            \"\"\"\n\n            formatted_data.append({\"text\": formatted_text})\n            # import pdb;pdb.set_trace()\n\n    # تبدیل به Dataset\n    dataset = Dataset.from_pandas(pd.DataFrame(formatted_data))\n\n    # تابع tokenization\n    def tokenize_function(examples):\n        return tokenizer(\n            examples[\"text\"],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=512  # یا مقدار مناسب برای مدل شما\n        )\n\n    # tokenize کردن داده‌ها\n    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n\n    # تقسیم داده به train و eval\n    tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n\n    return tokenized_dataset\n# torch.cuda.empty_cache()\n# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, token=use_auth_token)\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = \"right\"\n# dataset_path = \"/kaggle/input/multi-intend-detection/conversation_based_intend_database(method 1).json\"\n\n# tokenized_conversation_dataset = prepare_training_data_conversation(dataset_path, tokenizer)\n# print(tokenized_conversation_dataset)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":252,"referenced_widgets":["995029c99a6a45eeb9847dbda3db816f","b0ba72124aae4e1189365461fa932539","5f5b937c61a4447d9cc64ab520672e02","c195ab1ade6b456db214d08f57c02f72","4081d804c70144a1b51d466cd8936f8c","b5eae2ce3bee42dd970c88c988161cbd","d523d9022ce64aa481a57373f2e6a289","48ad6a6f3788414bb97087447cccebfe","43f04c6fab9e4929bcbbff1396d56d1b","f737a5b6f0db421abd6318aa439357e8","a2d149c299444115aace444f6b71b8ef"]},"id":"puOVqCUFkJBP","outputId":"1a8a46ea-e893-4d52-f2d0-5461d53fc026","trusted":true,"execution":{"iopub.status.busy":"2025-08-16T09:57:42.116085Z","iopub.execute_input":"2025-08-16T09:57:42.116574Z","iopub.status.idle":"2025-08-16T09:57:43.014682Z","shell.execute_reply.started":"2025-08-16T09:57:42.116556Z","shell.execute_reply":"2025-08-16T09:57:43.014048Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### for the second method\n","metadata":{"id":"V-9k1uOVy2X3"}},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom datasets import Dataset\n\ndef prepare_training_data_intent(data_path, tokenizer):\n    \"\"\"آماده‌سازی داده‌ها برای آموزش دیتاست تشخیص نیت پزشکی\"\"\"\n\n    print(\"آماده‌سازی داده‌های آموزشی...\")\n\n    # بارگذاری داده‌ها\n    with open(data_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n\n    # دسترسی به لیست Utterance\n    utterances = data.get(\"Utterance\", [])\n\n    # ایجاد فرمت مناسب برای آموزش\n    formatted_data = []\n\n    for item in utterances:\n        query = item.get(\"query\", \"\")\n        intends = item.get(\"intend\", [])\n        intends_text = \"، \".join(intends)\n        # پاسخ پیشنهادی: اعلام نیت‌ها\n        response = f\"نیت(های) کاربر: {intends_text}\"\n\n        formatted_text = f\"\"\"\n        ### سیستم:\n        شما یک دستیار پزشکی هوشمند هستید که نیت کاربر را از پرسش‌های پزشکی استخراج می‌کنید.\n\n        ### کاربر:\n        {query}\n\n        ### پاسخ:\n        {response}\n        \"\"\"\n        formatted_data.append({\"text\": formatted_text})\n        # import pdb;pdb.set_trace()\n    # تبدیل به Dataset\n    dataset = Dataset.from_pandas(pd.DataFrame(formatted_data))\n\n    # تابع tokenization\n    def tokenize_function(examples):\n        return tokenizer(\n            examples[\"text\"],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=512  # تنظیم بر اساس نیاز\n        )\n\n    # tokenize کردن داده‌ها\n    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n\n    # تقسیم داده به train و eval\n    tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n\n    return tokenized_dataset\n# torch.cuda.empty_cache()\n# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, token=use_auth_token)\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = \"right\"\n# dataset_path = \"/kaggle/input/multi-intend-detection/intend_database(method2).json\"\n\n# tokenized_intent_dataset = prepare_training_data_intent(dataset_path, tokenizer)\n# print(tokenized_intent_dataset)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0,"referenced_widgets":["473b705452e84904bc1177b5544a612f","7ce6814abc9e431b8a23ff4da9c1168f","3060c56c26b748c0b09181755ffee2e3","bfeeacd4082d4c41884e4ae4a746d3e3","aec800fcbe3045e99637ec6408ed3a0a","5a2a097dc176453db900157a0fabbb05","4dfbbbc2f5eb46219aaa96bb2cde4b04","ccaf32fa0dd442509f7f48e6becbe6e7","fbff2850b2f447b280a6c7993ca226cc","7c85ac3225e946d3b5e577970b3b979e","dfec3c1e16fd49cfa6bb9767d18485ce"]},"id":"XrM8b31aMn7A","outputId":"b5501bd9-1333-4d2b-97d0-f2de079dcc5e","trusted":true,"execution":{"iopub.status.busy":"2025-08-16T09:57:43.015625Z","iopub.execute_input":"2025-08-16T09:57:43.016257Z","iopub.status.idle":"2025-08-16T09:57:43.022513Z","shell.execute_reply.started":"2025-08-16T09:57:43.016231Z","shell.execute_reply":"2025-08-16T09:57:43.021911Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### for the third method","metadata":{"id":"Gjnj3rmkMjZY"}},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom datasets import Dataset\n\ndef prepare_training_data(data_path, tokenizer):\n    \"\"\"آماده‌سازی داده‌ها برای آموزش از فایل multi_intent_disease_queries\"\"\"\n\n    print(\"آماده‌سازی داده‌های آموزشی...\")\n\n    # بارگذاری داده‌ها\n    with open(data_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n\n    # دسترسی به لیست Utterance که حاوی query ها و diseases است\n    utterances = data.get(\"Utterance\", [])\n\n    # ایجاد فرمت مناسب برای آموزش\n    formatted_data = []\n\n    for item in utterances:\n        # استخراج query و لیست بیماری‌ها\n        query = item.get(\"query\", \"\")\n        diseases = item.get(\"diseases\", [])\n        #diseases = [\"بیماری مزمن کلیوی\", \"نارسایی قلبی\", \"پره‌اکلامپسی\"]\n        # import pdb;pdb.set_trace()\n        # تبدیل لیست بیماری‌ها به متن\n        diseases_text = \"، \".join(diseases)\n\n        # ایجاد پاسخ مناسب برای بیماری‌ها\n        response = f\"با توجه به علائم شما، بیماری‌های محتمل می‌توانند {diseases_text} باشد\"\n\n        # فرمت‌بندی متن برای مدل\n        formatted_text = f\"\"\"\n        ### سیستم:\n        شما یک دستیار پزشکی متخصص هستید. وظیفه شما تحلیل علائم کاربر و شناسایی بیماری‌های احتمالی است.\n\n        ### کاربر:\n        {query}\n\n        ### پاسخ:\n        {response}\n        \"\"\"\n\n        formatted_data.append({\"text\": formatted_text})\n\n    # تبدیل به Dataset\n    dataset = Dataset.from_pandas(pd.DataFrame(formatted_data))\n\n    # تابع tokenization\n    def tokenize_function(examples):\n        return tokenizer(\n            examples[\"text\"],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=512  # تنظیم بر اساس نیاز\n        )\n\n    # tokenize کردن داده‌ها\n    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n\n    # تقسیم داده به train و eval\n    tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n\n    return tokenized_dataset\n# torch.cuda.empty_cache()\n# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, token=use_auth_token)\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = \"right\"\n# dataset_path = \"/kaggle/input/multi-intend-detection/multi_intent_disease_queries(method3).json\"\n# tokenized_diseases_dataset = prepare_training_data(dataset_path, tokenizer)\n# print(tokenized_diseases_dataset)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":252,"referenced_widgets":["27722232f83240328f5cdd14f88ff6d8","2794f46dd35e4067801d9c110c9d7d77","7b904d600376469da5ca728f4285ece4","bd4a6060f0c84aa6bb69b9ca4e99633a","537ca4e20fd544e68f183576999a3357","d05ca9e806064d239a920ceb24ca7471","a0ddb6f740e845c6bd23f885537223b1","ac5e006997e04317b947f6791623c9c4","cb7cccc9c1614f2a887b0438d5bc1289","184ccc0bc81649b191ab1fd165df0e9b","35f2ed0a745f46f0aff4e5846736ba1b"]},"id":"4HLKT-mly7c8","outputId":"29a3e9e8-09f7-44ae-ea44-8280f63dae98","trusted":true,"execution":{"iopub.status.busy":"2025-08-16T09:57:43.023383Z","iopub.execute_input":"2025-08-16T09:57:43.023601Z","iopub.status.idle":"2025-08-16T09:57:43.048620Z","shell.execute_reply.started":"2025-08-16T09:57:43.023585Z","shell.execute_reply":"2025-08-16T09:57:43.048115Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### for the general tasks","metadata":{"id":"RIoF1NQuMeTK"}},{"cell_type":"code","source":"# import json\n# import pandas as pd\n# from datasets import Dataset\n\n# def prepare_training_data(data_path, tokenizer):\n#     \"\"\"آماده‌سازی داده‌ها برای آموزش\"\"\"\n\n#     print(\"آماده‌سازی داده‌های آموزشی...\")\n\n#     # بارگذاری داده‌ها\n#     with open(data_path, 'r', encoding='utf-8') as f:\n#         conversations = json.load(f)\n\n#     # ایجاد فرمت مناسب برای آموزش\n#     formatted_data = []\n\n#     for item in conversations:\n#         # استخراج مکالمه و اطلاعات بیماری\n#         conversation = item['conversation']\n#         disease_info = item.get('metadata', {})\n\n\n#         if 'بیمار: ' in conversation:\n#             user_part = conversation.split('بیمار: ')[1].split('پزشک: ')[0]\n#         else:\n#             user_part = conversation.split('\\n')[0]\n\n#         if 'پزشک: ' in conversation:\n#             response_part = conversation.split('پزشک: ')[1].split('بیمار: ')[0]\n#         else:\n#             response_part = \"متاسفانه فرمت مکالمه استاندارد نیست.\"\n\n\n#         formatted_text = f\"\"\"\n#         ### سیستم:\n#         شما یک دستیار پزشکی دانا هستید که اطلاعات دقیق و کاربردی در مورد بیماری‌ها ارائه می‌دهید.\n\n#         ### کاربر:\n#         {user_part}\n\n#         ### پاسخ:\n#         {response_part}\n#         \"\"\"\n#         formatted_data.append({\"text\": formatted_text})\n\n#     # تبدیل به Dataset\n#     dataset = Dataset.from_pandas(pd.DataFrame(formatted_data))\n\n#     # تابع tokenization\n#     def tokenize_function(examples):\n#         return tokenizer(\n#             examples[\"text\"],\n#             padding=\"max_length\",\n#             truncation=True,\n#             max_length=512  # تنظیم بر اساس نیاز\n#         )\n\n#     # tokenize کردن داده‌ها\n#     tokenized_dataset = dataset.map(tokenize_function, batched=True)\n\n#     # تقسیم داده به train و eval\n#     tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n\n#     return tokenized_dataset","metadata":{"id":"WAsuycuEnc1G","trusted":true,"execution":{"iopub.status.busy":"2025-08-16T09:57:43.050481Z","iopub.execute_input":"2025-08-16T09:57:43.050670Z","iopub.status.idle":"2025-08-16T09:57:43.071050Z","shell.execute_reply.started":"2025-08-16T09:57:43.050656Z","shell.execute_reply":"2025-08-16T09:57:43.070378Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## تنظیم پارامترهای LoRA","metadata":{"id":"0ckorfXOwgzp"}},{"cell_type":"code","source":"from peft import LoraConfig\n\ndef create_lora_config():\n    \"\"\"تنظیم پارامترهای LoRA\"\"\"\n\n    # پیکربندی LoRA\n    # lora_config = LoraConfig(\n    #     # r=16,                     # رتبه ماتریس‌های LoRA\n    #     lora_alpha=32,            # پارامتر مقیاس‌دهی\n    #     lora_dropout=0.05,        # نرخ dropout\n    #     bias=\"none\",              # آموزش بایاس\n    #     task_type=\"CAUSAL_LM\",    # نوع وظیفه\n    #     target_modules=[          # لایه‌های هدف برای اعمال LoRA\n    #         \"q_proj\",\n    #         \"k_proj\",\n    #         \"v_proj\",\n    #         \"o_proj\"\n    #     ]\n    # )\n    lora_config = LoraConfig(\n        r=4,  # Further reduce from 8 to 4 (or even 2)\n        lora_alpha=16,  # Reduce proportionally (r * 2)\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules=[\n            \"q_proj\",\n            # \"v_proj\"  # Remove k_proj and o_proj\n        ]\n    )\n\n\n    return lora_config","metadata":{"id":"S3wrtJzonfuH","trusted":true,"execution":{"iopub.status.busy":"2025-08-16T09:57:43.071782Z","iopub.execute_input":"2025-08-16T09:57:43.072435Z","iopub.status.idle":"2025-08-16T09:57:43.096376Z","shell.execute_reply.started":"2025-08-16T09:57:43.072418Z","shell.execute_reply":"2025-08-16T09:57:43.095820Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## اجرای Fine-tuning","metadata":{"id":"GfWcvOh6wpFb"}},{"cell_type":"code","source":"# from transformers import TrainingArguments, Trainer\n# import torch\n# from peft import get_peft_model, LoraConfig, TaskType\n\n# def train_model(model, tokenizer, tokenized_dataset, lora_config):\n#     \"\"\"اجرای فرایند آموزش\"\"\"\n\n#     print(\"آماده‌سازی مدل LoRA...\")\n#     # اعمال LoRA به مدل پایه\n#     peft_model = get_peft_model(model, lora_config)\n\n#     # نمایش پارامترهای قابل آموزش\n#     peft_model.print_trainable_parameters()\n\n#     # تنظیم پارامترهای آموزش\n#     training_args = TrainingArguments(\n#         output_dir=\"./medical_lora_output\",\n#         num_train_epochs=3,                  # تعداد دوره‌ها\n#         per_device_train_batch_size=4,       # اندازه batch\n#         gradient_accumulation_steps=8,       # تجمیع گرادیان\n#         gradient_checkpointing=True,         # کاهش مصرف حافظه\n#         save_strategy=\"epoch\",               # ذخیره در هر دوره\n#         logging_steps=10,                    # گزارش هر 10 قدم\n#         learning_rate=2e-4,                  # نرخ یادگیری\n#         fp16=True,                           # استفاده از precision نیم‌دقت\n#         load_best_model_at_end=True,         # بارگذاری بهترین مدل در پایان\n#         evaluation_strategy=\"epoch\",         # ارزیابی در هر دوره\n#     )\n\n#     # تعریف Trainer\n#     trainer = Trainer(\n#         model=peft_model,\n#         args=training_args,\n#         train_dataset=tokenized_dataset[\"train\"],\n#         eval_dataset=tokenized_dataset[\"test\"],\n#         tokenizer=tokenizer,\n#     )\n\n#     print(\"شروع آموزش...\")\n#     trainer.train()\n\n#     print(\"آموزش به پایان رسید. ذخیره مدل...\")\n#     peft_model.save_pretrained(\"./final_medical_lora\")\n#     tokenizer.save_pretrained(\"./final_medical_lora\")\n\n#     return peft_model","metadata":{"id":"HwrdSFw_wqne","trusted":true,"execution":{"iopub.status.busy":"2025-08-16T09:57:43.096908Z","iopub.execute_input":"2025-08-16T09:57:43.097146Z","iopub.status.idle":"2025-08-16T09:57:43.121706Z","shell.execute_reply.started":"2025-08-16T09:57:43.097110Z","shell.execute_reply":"2025-08-16T09:57:43.120642Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import gc\nimport torch\nfrom transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom peft import get_peft_model, LoraConfig, TaskType\n\ndef train_model(model, tokenizer, tokenized_dataset, lora_config):\n    \"\"\"اجرای فرایند آموزش بهینه‌شده\"\"\"\n\n    # 🔧 IMPROVEMENT 1: Memory cleanup before starting\n    print(\"پاک‌سازی حافظه...\")\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # 🔧 IMPROVEMENT 2: Memory monitoring function\n    def print_memory_usage(stage):\n        if torch.cuda.is_available():\n            allocated = torch.cuda.memory_allocated() / 1024**3\n            reserved = torch.cuda.memory_reserved() / 1024**3\n            print(f\"[{stage}] GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n\n    print_memory_usage(\"شروع\")\n    \n    print(\"آماده‌سازی مدل LoRA...\")\n    \n    # 🔧 IMPROVEMENT 3: Error handling for PEFT model creation\n    try:\n        model.gradient_checkpointing_enable()\n        peft_model = get_peft_model(model, lora_config)\n        print_memory_usage(\"بعد از ایجاد PEFT\")\n    except RuntimeError as e:\n        print(f\"خطا در ایجاد مدل PEFT: {e}\")\n        # Clear cache and try with CPU offloading\n        torch.cuda.empty_cache()\n        peft_model = get_peft_model(model, lora_config)\n        \n        # Move some layers to CPU to save GPU memory\n        if hasattr(peft_model.base_model.model, 'embed_tokens'):\n            peft_model.base_model.model.embed_tokens = peft_model.base_model.model.embed_tokens.cpu()\n        if hasattr(peft_model.base_model.model, 'norm'):\n            peft_model.base_model.model.norm = peft_model.base_model.model.norm.cpu()\n\n    # نمایش پارامترهای قابل آموزش\n    peft_model.print_trainable_parameters()\n\n    # 🔧 IMPROVEMENT 4: Fixed training arguments for newer transformers versions\n    training_args = TrainingArguments(\n        output_dir=\"./medical_lora_output\",\n        num_train_epochs=3,\n        per_device_train_batch_size=1,       #   Reduced from 4 to 1 for memory\n        gradient_accumulation_steps=32,      #   Increased from 8 to 32 to maintain effective batch size\n        gradient_checkpointing=True,\n        save_strategy=\"epoch\",\n        logging_steps=10,\n        learning_rate=2e-4,\n        fp16=True,\n        load_best_model_at_end=False,        #   Disabled to save memory\n        eval_strategy=\"steps\",               #   FIXED: Changed from evaluation_strategy to eval_strategy\n        eval_steps=500,                      #   Evaluate every 500 steps instead of every epoch\n        \n        # 🔧 IMPROVEMENT 5: Additional memory optimizations\n        dataloader_pin_memory=False,         #   Disable pin memory to save RAM\n        remove_unused_columns=True,          #   Remove unused dataset columns\n        save_total_limit=2,                  #   Keep only 2 checkpoints\n        prediction_loss_only=True,           #  Only compute loss during eval\n        \n        # 🔧 IMPROVEMENT 6: Optimizer and scheduler optimizations\n        optim=\"adamw_torch\",                 #   Use PyTorch's AdamW (more memory efficient)\n        max_grad_norm=1.0,                   #   Gradient clipping\n        warmup_steps=100,                    #   Learning rate warmup\n        lr_scheduler_type=\"cosine\",          #   Better learning rate schedule\n        \n        # 🔧 IMPROVEMENT 7: Logging and monitoring\n        report_to=\"none\",                    #   Disable wandb/tensorboard to save memory\n        logging_first_step=True,\n        \n        # 🔧 IMPROVEMENT 8: Advanced memory management - removed problematic arguments\n        include_inputs_for_metrics=False,    #   Don't include inputs in metrics\n    )\n\n    # 🔧 IMPROVEMENT 9: Data collator for better memory efficiency\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False,  # We're doing causal LM, not masked LM\n        pad_to_multiple_of=8,  # Pad to multiple of 8 for tensor cores\n    )\n\n    # 🔧 IMPROVEMENT 10: Simplified callback for memory monitoring\n    from transformers import TrainerCallback\n    \n    class MemoryCallback(TrainerCallback):\n        def on_step_end(self, args, state, control, model=None, **kwargs):\n            if state.global_step % 50 == 0:  # Every 50 steps\n                print_memory_usage(f\"Step {state.global_step}\")\n                torch.cuda.empty_cache()  # Clear cache periodically\n\n    # تعریف Trainer با تنظیمات بهینه\n    trainer = Trainer(\n        model=peft_model,\n        args=training_args,\n        train_dataset=tokenized_dataset[\"train\"],\n        eval_dataset=tokenized_dataset.get(\"test\", None),  #   Handle missing test set\n        tokenizer=tokenizer,\n        data_collator=data_collator,           #  Use efficient data collator\n        callbacks=[MemoryCallback()],         #  Memory monitoring\n    )\n\n    print(\"شروع آموزش...\")\n    print_memory_usage(\"قبل از آموزش\")\n    \n    # 🔧 IMPROVEMENT 11: Training with error handling\n    try:\n        training_result = trainer.train()\n        print(f\"آموزش کامل شد. Final loss: {training_result.training_loss:.4f}\")\n    except RuntimeError as e:\n        if \"out of memory\" in str(e).lower():\n            print(\"خطای کمبود حافظه! تلاش با batch size کمتر...\")\n            torch.cuda.empty_cache()\n            # Reduce batch size further and try again\n            training_args.per_device_train_batch_size = 1\n            training_args.gradient_accumulation_steps = 64\n            \n            trainer = Trainer(\n                model=peft_model,\n                args=training_args,\n                train_dataset=tokenized_dataset[\"train\"],\n                eval_dataset=tokenized_dataset.get(\"test\", None),\n                tokenizer=tokenizer,\n                data_collator=data_collator,\n            )\n            training_result = trainer.train()\n        else:\n            raise e\n\n    print_memory_usage(\"بعد از آموزش\")\n\n    # 🔧 IMPROVEMENT 12: Optimized model saving\n    print(\"آموزش به پایان رسید. ذخیره مدل...\")\n    \n    # Clear cache before saving\n    torch.cuda.empty_cache()\n    \n    # Save with safe serialization\n    peft_model.save_pretrained(\n        \"./final_medical_lora\",\n        safe_serialization=True,  #   Use safe tensors format\n        max_shard_size=\"2GB\"      #   Split large models into smaller shards\n    )\n    tokenizer.save_pretrained(\"./final_medical_lora\")\n    \n    # 🔧 IMPROVEMENT 13: Generate training summary\n    print(\"\\n📊 خلاصه آموزش:\")\n    print(f\"تعداد کل قدم‌ها: {trainer.state.global_step}\")\n    print(f\"تعداد epochs: {trainer.state.epoch}\")\n    if hasattr(training_result, 'training_loss'):\n        print(f\"Loss نهایی: {training_result.training_loss:.4f}\")\n    \n    print_memory_usage(\"پایان\")\n    \n    # Final cleanup\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return peft_model, trainer.state.log_history  #   Return training history too","metadata":{"id":"Svh77yJ6x3RL","trusted":true,"execution":{"iopub.status.busy":"2025-08-16T09:57:43.122714Z","iopub.execute_input":"2025-08-16T09:57:43.123015Z","iopub.status.idle":"2025-08-16T09:57:44.692570Z","shell.execute_reply.started":"2025-08-16T09:57:43.122989Z","shell.execute_reply":"2025-08-16T09:57:44.692014Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import bitsandbytes as bnb\nprint(bnb.__version__)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9cd8chEwzIWK","outputId":"3037aa98-e81d-427b-d029-25f833ba5a28","trusted":true,"execution":{"iopub.status.busy":"2025-08-16T09:57:44.693289Z","iopub.execute_input":"2025-08-16T09:57:44.693551Z","iopub.status.idle":"2025-08-16T09:57:47.480537Z","shell.execute_reply.started":"2025-08-16T09:57:44.693526Z","shell.execute_reply":"2025-08-16T09:57:47.479887Z"}},"outputs":[{"name":"stdout","text":"0.47.0\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch\nprint(torch.version.cuda)  # CUDA version PyTorch was built with\nprint(torch.cuda.is_available())\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HK-p-KVgzMPE","outputId":"14b89640-0b95-4828-8881-9d41b2e68dc3","trusted":true,"execution":{"iopub.status.busy":"2025-08-16T09:57:47.481272Z","iopub.execute_input":"2025-08-16T09:57:47.481525Z","iopub.status.idle":"2025-08-16T09:57:47.485689Z","shell.execute_reply.started":"2025-08-16T09:57:47.481501Z","shell.execute_reply":"2025-08-16T09:57:47.485028Z"}},"outputs":[{"name":"stdout","text":"12.4\nTrue\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T09:57:47.486472Z","iopub.execute_input":"2025-08-16T09:57:47.486796Z","iopub.status.idle":"2025-08-16T09:57:47.540104Z","shell.execute_reply.started":"2025-08-16T09:57:47.486771Z","shell.execute_reply":"2025-08-16T09:57:47.539386Z"}},"outputs":[{"name":"stdout","text":"4.55.2\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## run functions","metadata":{"id":"LhkG3YH0w8xO"}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel, PeftConfig\n\ndef main():\n    torch.cuda.empty_cache()\n    model, tokenizer = load_base_model()\n\n‍‍    dataset_path = \"/kaggle/input/multi-intend-detection/multi_intent_disease_queries(method3).json\"\n    tokenized_dataset = prepare_training_data(dataset_path, tokenizer)\n\n    # تنظیم LoRA\n    lora_config = create_lora_config()\n\n    # آموزش مدل\n    trained_model = train_model(model, tokenizer, tokenized_dataset, lora_config)\n\n    # تست مدل\n    test_model(\"./final_medical_lora\")\n\ndef test_model(model_path):\n    print(\"تست مدل آموزش‌دیده...\")\n\n    # بارگذاری تنظیمات LoRA\n    config = PeftConfig.from_pretrained(model_path)\n\n    # بارگذاری مدل پایه\n    base_model = AutoModelForCausalLM.from_pretrained(\n        config.base_model_name_or_path,\n        load_in_8bit=True,\n        device_map=\"auto\"\n    )\n\n    # بارگذاری tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    # ترکیب مدل پایه و LoRA\n    model = PeftModel.from_pretrained(base_model, model_path)\n\n    # نمونه سوالات برای تست\n    test_questions = [\n        \"علائم سرماخوردگی چیست؟\",\n        \"فشار خون بالا چگونه درمان می‌شود؟\",\n        \"برای پیشگیری از بیماری قلبی چه باید کرد؟\"\n    ]\n\n    for question in test_questions:\n        prompt = f\"\"\"### سیستم:\nشما یک دستیار پزشکی دانا هستید که اطلاعات دقیق و کاربردی در مورد بیماری‌ها ارائه می‌دهید.\n\n### کاربر:\n{question}\n\n### پاسخ:\n\"\"\"\n\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n        # تولید پاسخ\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=512,\n                temperature=0.7,\n                do_sample=True,\n                top_p=0.9\n            )\n\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        print(f\"\\nسوال: {question}\")\n        print(f\"پاسخ: {response.split('### پاسخ:')[1].strip()}\")\n        print(\"-\" * 50)\n\n# if __name__ == \"__main__\":\n    # main()\n\n# بارگذاری مدل پایه\ntorch.cuda.empty_cache()\nmodel, tokenizer = load_base_model()\n\n# آماده‌سازی داده‌ها\ndataset_path = \"/kaggle/input/multi-intend-detection/multi_intent_disease_queries(method3).json\"\ntokenized_diseases_dataset = prepare_training_data(dataset_path, tokenizer)\nprint(tokenized_diseases_dataset)\n\n# dataset_path = \"/kaggle/input/multi-intend-detection/intend_database(method2).json\"\n# tokenized_intent_dataset = prepare_training_data_intent(dataset_path, tokenizer)\n# print(tokenized_intent_dataset)\n\n# dataset_path = \"/kaggle/input/multi-intend-detection/conversation_based_intend_database(method 1).json\"\n# tokenized_conversation_dataset = prepare_training_data_conversation(dataset_path, tokenizer)\n# print(tokenized_conversation_dataset)\n\n# تنظیم LoRA\nlora_config = create_lora_config()\n\n# آموزش مدل\ntrained_model = train_model(model, tokenizer, tokenized_diseases_dataset, lora_config)\n\n# تست مدل\n# test_model(\"./final_medical_lora\")","metadata":{"id":"Hm66itM6wrMF","colab":{"base_uri":"https://localhost:8080/","height":541},"outputId":"be1ad75b-32c6-4a21-930b-f600f87ee1f6","trusted":true,"execution":{"iopub.status.busy":"2025-08-16T09:57:47.540817Z","iopub.execute_input":"2025-08-16T09:57:47.541098Z","iopub.status.idle":"2025-08-16T15:12:05.052537Z","shell.execute_reply.started":"2025-08-16T09:57:47.541082Z","shell.execute_reply":"2025-08-16T15:12:05.051868Z"}},"outputs":[{"name":"stdout","text":"بارگذاری مدل CohereLabs/aya-23-8B...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/9.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41c645291c1e4d9e94418227ebd953e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/16.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77f83c328d7441d188ded546f5c76cf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/640 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d71bae00e3a46429c1affd93b6103f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/21.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"471a3fbed8de4cb9ae4c70a8b9edb9e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8c7beda9e424d4cb3f58f9e974d6531"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3976eef231814a69ae590df757635bb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e9bfeb043e641ab8217539923886b73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e0e4b3f44af4c7eb178e1569e0da909"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3239c45f356410ea9e4e731de5e188b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bba83a4e62e6443ea7c9cd1585c8d90b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f054ec48e6f44bd3bec8d4b088303b25"}},"metadata":{}},{"name":"stdout","text":"آماده‌سازی داده‌های آموزشی...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2454 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2475dc145b4465e8793ea554556e33b"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'input_ids', 'attention_mask'],\n        num_rows: 2208\n    })\n    test: Dataset({\n        features: ['text', 'input_ids', 'attention_mask'],\n        num_rows: 246\n    })\n})\nپاک‌سازی حافظه...\n[شروع] GPU Memory - Allocated: 4.22GB, Reserved: 5.50GB\nآماده‌سازی مدل LoRA...\n[بعد از ایجاد PEFT] GPU Memory - Allocated: 4.22GB, Reserved: 5.50GB\ntrainable params: 1,048,576 || all params: 8,029,081,600 || trainable%: 0.0131\nشروع آموزش...\n[قبل از آموزش] GPU Memory - Allocated: 4.22GB, Reserved: 5.50GB\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3579262723.py:97: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='207' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [207/207 5:11:39, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"[Step 50] GPU Memory - Allocated: 4.24GB, Reserved: 7.95GB\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-68a06ed8-33169ab678ad4876740978b7;561243d9-52dd-47fd-ba5b-dfb9cb27d103)\n\nCannot access gated repo for url https://huggingface.co/CohereLabs/aya-23-8B/resolve/main/config.json.\nAccess to model CohereLabs/aya-23-8B is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in CohereLabs/aya-23-8B.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in CohereLabs/aya-23-8B - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[Step 100] GPU Memory - Allocated: 4.24GB, Reserved: 7.95GB\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-68a0874d-1e3785106c2c839d14322216;b68e6141-3a7e-44a9-a48d-65f264b8a2b9)\n\nCannot access gated repo for url https://huggingface.co/CohereLabs/aya-23-8B/resolve/main/config.json.\nAccess to model CohereLabs/aya-23-8B is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in CohereLabs/aya-23-8B.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in CohereLabs/aya-23-8B - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[Step 150] GPU Memory - Allocated: 4.24GB, Reserved: 7.95GB\n[Step 200] GPU Memory - Allocated: 4.24GB, Reserved: 7.95GB\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-68a09fc4-0e5e7ac623a78fd74d2ec932;6995ac5b-b9fc-43b0-8066-530c73325cdc)\n\nCannot access gated repo for url https://huggingface.co/CohereLabs/aya-23-8B/resolve/main/config.json.\nAccess to model CohereLabs/aya-23-8B is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in CohereLabs/aya-23-8B.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in CohereLabs/aya-23-8B - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"آموزش کامل شد. Final loss: 1.0594\n[بعد از آموزش] GPU Memory - Allocated: 4.24GB, Reserved: 7.95GB\nآموزش به پایان رسید. ذخیره مدل...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-68a09fc4-09070bd11781906d6c8da7d0;abfa4ed1-290c-4fab-9b92-1d648d6b4ca0)\n\nCannot access gated repo for url https://huggingface.co/CohereLabs/aya-23-8B/resolve/main/config.json.\nAccess to model CohereLabs/aya-23-8B is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in CohereLabs/aya-23-8B.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in CohereLabs/aya-23-8B - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n📊 خلاصه آموزش:\nتعداد کل قدم‌ها: 207\nتعداد epochs: 3.0\nLoss نهایی: 1.0594\n[پایان] GPU Memory - Allocated: 4.24GB, Reserved: 5.50GB\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import os\nimport zipfile\n\n# Define the folder to compress\nfolder_path = '/kaggle/working/medical_lora_output'\noutput_zip = '/kaggle/working/medical_lora_output.zip'\n\n# Create a zip file\nwith zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for root, dirs, files in os.walk(folder_path):\n        for file in files:\n            zipf.write(os.path.join(root, file), \n                      os.path.relpath(os.path.join(root, file), folder_path))\n\n# Generate download link\nfrom IPython.display import FileLink\nFileLink(output_zip)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T15:49:57.467214Z","iopub.execute_input":"2025-08-16T15:49:57.467498Z","iopub.status.idle":"2025-08-16T15:49:59.948572Z","shell.execute_reply.started":"2025-08-16T15:49:57.467479Z","shell.execute_reply":"2025-08-16T15:49:59.947943Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/medical_lora_output.zip","text/html":"<a href='/kaggle/working/medical_lora_output.zip' target='_blank'>/kaggle/working/medical_lora_output.zip</a><br>"},"metadata":{}}],"execution_count":18}]}